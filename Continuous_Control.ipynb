{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Reacher.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "# Implement multivariate Gaussian policy with fixed scale\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Policy, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "\n",
    "        # Create an output layer for the means\n",
    "        self.mean = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.tanh(self.fc1(state))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "\n",
    "        mu = self.mean(x)        \n",
    "        return mu, torch.zeros(action_size)\n",
    "    \n",
    "# Construct the policy model\n",
    "policy = Policy(state_size, action_size).to(device)\n",
    "\n",
    "# Use the Adam optimizer with learning rate 2.5e-4\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(policy.parameters(), lr=2.5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. PPO\n",
    "\n",
    "Implement the PPO algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode length is 1001, average reward is 1.6519999630749225, total timesteps is 5350\n",
      "Episode length is 1001, average reward is 2.3404999476857484, total timesteps is 10755\n",
      "Episode length is 1001, average reward is 4.1464999073185025, total timesteps is 16095\n",
      "Episode length is 1001, average reward is 4.467999900132417, total timesteps is 21370\n",
      "Episode length is 1001, average reward is 5.8414998694323, total timesteps is 26700\n",
      "Episode length is 1001, average reward is 5.8704998687840995, total timesteps is 31965\n",
      "Episode length is 1001, average reward is 7.033499842789024, total timesteps is 37165\n",
      "Episode length is 1001, average reward is 8.202499816659838, total timesteps is 42420\n",
      "Episode length is 1001, average reward is 8.046999820135534, total timesteps is 47850\n",
      "Episode length is 1001, average reward is 9.182999794743955, total timesteps is 53215\n",
      "Episode length is 1001, average reward is 10.376999768055976, total timesteps is 58515\n",
      "Episode length is 1001, average reward is 10.985999754443764, total timesteps is 63870\n",
      "Episode length is 1001, average reward is 12.337499724235386, total timesteps is 69160\n",
      "Episode length is 1001, average reward is 12.116499729175121, total timesteps is 74385\n",
      "Episode length is 1001, average reward is 13.19049970516935, total timesteps is 79665\n",
      "Episode length is 1001, average reward is 15.073999663069845, total timesteps is 84880\n",
      "Episode length is 1001, average reward is 16.30849963547662, total timesteps is 90270\n",
      "Episode length is 1001, average reward is 17.021499619539828, total timesteps is 95595\n",
      "Episode length is 1001, average reward is 18.441499587800354, total timesteps is 100975\n",
      "Episode length is 1001, average reward is 16.150499639008196, total timesteps is 106290\n",
      "Episode length is 1001, average reward is 17.166499616298825, total timesteps is 111540\n",
      "Episode length is 1001, average reward is 20.67649953784421, total timesteps is 116845\n",
      "Episode length is 1001, average reward is 18.845999578759074, total timesteps is 122085\n",
      "Episode length is 1001, average reward is 22.02199950776994, total timesteps is 127260\n",
      "Episode length is 1001, average reward is 23.196999481506644, total timesteps is 132610\n",
      "Episode length is 1001, average reward is 22.054999507032335, total timesteps is 138015\n",
      "Episode length is 1001, average reward is 23.918499465379863, total timesteps is 143355\n",
      "Episode length is 1001, average reward is 22.359999500215054, total timesteps is 148630\n",
      "Episode length is 1001, average reward is 24.909499443229286, total timesteps is 153960\n",
      "Episode length is 1001, average reward is 23.315999478846788, total timesteps is 159225\n",
      "Episode length is 1001, average reward is 24.633499449398368, total timesteps is 164425\n",
      "Episode length is 1001, average reward is 26.69349940335378, total timesteps is 169680\n",
      "Episode length is 1001, average reward is 25.791999423503874, total timesteps is 175110\n",
      "Episode length is 1001, average reward is 26.31799941174686, total timesteps is 180475\n",
      "Episode length is 1001, average reward is 27.35099938865751, total timesteps is 185775\n",
      "Episode length is 1001, average reward is 27.60199938304722, total timesteps is 191130\n",
      "Episode length is 1001, average reward is 25.73199942484498, total timesteps is 196420\n",
      "Episode length is 1001, average reward is 30.77299931216985, total timesteps is 201645\n",
      "Episode length is 1001, average reward is 28.11499937158078, total timesteps is 206925\n",
      "Episode length is 1001, average reward is 28.773499356862157, total timesteps is 212140\n",
      "Episode length is 1001, average reward is 26.116499416250736, total timesteps is 217530\n",
      "Episode length is 1001, average reward is 26.248499413300305, total timesteps is 222855\n",
      "Episode length is 1001, average reward is 27.82299937810749, total timesteps is 228235\n",
      "Episode length is 1001, average reward is 30.847499310504645, total timesteps is 233550\n",
      "Episode length is 1001, average reward is 28.693499358650296, total timesteps is 238800\n",
      "Episode length is 1001, average reward is 30.307499322574586, total timesteps is 244105\n",
      "Episode length is 1001, average reward is 28.26149936830625, total timesteps is 249345\n",
      "Episode length is 1001, average reward is 30.72949931314215, total timesteps is 254520\n",
      "Episode length is 1001, average reward is 33.60949924876913, total timesteps is 259870\n",
      "Episode length is 1001, average reward is 29.77649933444336, total timesteps is 265275\n",
      "Episode length is 1001, average reward is 32.01299928445369, total timesteps is 270615\n",
      "Episode length is 1001, average reward is 32.03149928404018, total timesteps is 275890\n",
      "Episode length is 1001, average reward is 32.35399927683174, total timesteps is 281220\n",
      "Episode length is 1001, average reward is 31.868499287683516, total timesteps is 286485\n",
      "Episode length is 1001, average reward is 29.030999351106583, total timesteps is 291685\n",
      "Episode length is 1001, average reward is 32.85149926571175, total timesteps is 296940\n",
      "\n",
      "Environment solved in 183 episodes!\tAverage Score: 30.02\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "num_epochs = 7\n",
    "\n",
    "def sample_trajectory_segment(policy, env_info, horizon):\n",
    "    \"\"\"\n",
    "    Sample a trajectory segment\n",
    "\n",
    "    :param (Policy) policy\n",
    "    :param (BrainInfo) env_info\n",
    "    :param horizon (int) the number of timesteps to sample\n",
    "    :return: (dict) generator with the following keys:\n",
    "    \n",
    "        - ob: (np.array) observations\n",
    "        - ac: (np.array) actions\n",
    "        - rew: (float) rewards\n",
    "        - logprob: (np.array) log probabilities of actions\n",
    "        - done: True if end of episode (one of more)\n",
    "        - ep_rets: (float) cumulated current episode reward\n",
    "        - ep_len: (int) the length of the current episode\n",
    "        - total_timesteps: (int) number of timesteps across horizon\n",
    "    \"\"\"\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    observation = np.asarray(env_info.vector_observations, dtype=np.float32)  # shape (20, 33)\n",
    "    ob, ac, rew, logprob = [], [], [], []\n",
    "    cur_ep_ret = 0  # return in current episode\n",
    "    cur_ep_len = 0  # len of current episode\n",
    "    ep_rets = []    # returns of completed episodes in this segment\n",
    "    ep_lens = []    # lengths of completed episodes in this segment\n",
    "    step = 0\n",
    "    done = False\n",
    "\n",
    "    while True:\n",
    "        if step > 0 and step % horizon == 0:\n",
    "            total_timesteps = sum(ep_lens) + cur_ep_len\n",
    "            yield {\"ob\": ob, \"ac\": ac, \"rew\": rew, \"logprob\": logprob,\n",
    "                   \"done\": done, \"ep_rets\": ep_rets, \"ep_lens\": ep_lens,\n",
    "                   \"total_timesteps\": total_timesteps}\n",
    "            ob, ac, rew, logprob = [], [], [], []\n",
    "            ep_rets = []  # returns of completed episodes in this segment\n",
    "            ep_lens = []  # lengths of completed episodes in this segment\n",
    "            done = False\n",
    "\n",
    "        ob.append(observation)\n",
    "\n",
    "        # Get probability distribution associated with state\n",
    "        with torch.no_grad():\n",
    "            mean, log_std = policy(torch.from_numpy(observation))\n",
    "            scale = torch.diag(torch.exp(log_std))\n",
    "            m = MultivariateNormal(mean, scale_tril=scale)\n",
    "            \n",
    "            # Use the \"reparameterization trick\" to sample an action\n",
    "            # for each agent:\n",
    "            #     mu + sigma * z,         z ~ N(0, I)\n",
    "            action = m.rsample()  # shape (20, 4)\n",
    "            p = m.log_prob(action)\n",
    "            action = action.numpy()\n",
    "            ac.append(action)\n",
    "            logprob.append(p.numpy())\n",
    "\n",
    "        # Clip the action to avoid out of bounds error\n",
    "        clipped_action = np.clip(action, -1, 1)\n",
    "        \n",
    "        # Take a step and get a reward\n",
    "        env_info = env.step(clipped_action)[brain_name]\n",
    "        rew.append(env_info.rewards)\n",
    "        observation = np.asarray(env_info.vector_observations, dtype=np.float32)\n",
    "        dones = env_info.local_done\n",
    "        \n",
    "        cur_ep_ret += sum(env_info.rewards)\n",
    "        cur_ep_len += 1\n",
    "        if np.any(dones):\n",
    "            ep_rets.append(cur_ep_ret)\n",
    "            ep_lens.append(cur_ep_len)\n",
    "            cur_ep_ret = 0\n",
    "            cur_ep_len = 0\n",
    "            env_info = env.reset(train_mode=True)[brain_name]\n",
    "            done = True\n",
    "        step += 1\n",
    "\n",
    "def discounted_rewards_to_go(rewards, discount):\n",
    "    rewards = np.stack(rewards)  # shape (T, num_agents)\n",
    "    T = rewards.shape[0]\n",
    "    discounts = discount**np.arange(T).reshape(T, -1)\n",
    "    discounted_rewards = rewards * discounts\n",
    "        \n",
    "    # Compute the \"rewards to go\" (future rewards) using reverse cumulative sum\n",
    "    future_rewards = discounted_rewards[::-1,...].cumsum(axis=0)[::-1,...]\n",
    "    return future_rewards.flatten()\n",
    "\n",
    "def surrogate_loss(policy, obs, actions, adv, old_logp, epsilon, ent_coef):\n",
    "    # Calculate log probabilities using current policy\n",
    "    mean, log_std = policy(torch.from_numpy(obs))\n",
    "    scale = torch.diag(torch.exp(log_std))\n",
    "    m = MultivariateNormal(mean, scale_tril=scale)\n",
    "    logp = m.log_prob(torch.from_numpy(actions))\n",
    "    ent = m.entropy()\n",
    "    ent_loss = torch.mean(ent) * ent_coef\n",
    "    \n",
    "    # Convert to torch tensors\n",
    "    adv = torch.tensor(adv.squeeze(), dtype=torch.float32)\n",
    "    \n",
    "    # Ratio for clipping\n",
    "    ratio = torch.exp(logp - torch.from_numpy(old_logp))\n",
    "\n",
    "    # Compute the policy loss\n",
    "    surr1 = ratio * adv\n",
    "    surr2 = torch.clamp(ratio, 1.0 - epsilon, 1.0 + epsilon) * adv    \n",
    "    policy_loss = torch.mean(torch.min(surr1, surr2))\n",
    "\n",
    "    # Compute the total loss\n",
    "    return policy_loss - ent_loss\n",
    "    \n",
    "def learn(max_timesteps, horizon, batch_size, discount=1.0, print_every=10, seed=5):\n",
    "    total_episodes = 0\n",
    "    total_timesteps = 0\n",
    "    epsilon = 0.1\n",
    "    ent_coef = 0.01\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Multiplier for annealing clipping param\n",
    "    mult = max(1.0 - float(total_timesteps) / max_timesteps, 0.0)\n",
    "\n",
    "    # Sample from the old policy\n",
    "    seg_gen = sample_trajectory_segment(policy, env_info, horizon)\n",
    "    \n",
    "    # Keep track of the mean rewards across the agents per episode\n",
    "    mean_rewards = []\n",
    "    \n",
    "    # Rolling buffer for episode rewards\n",
    "    rewards_buffer = deque(maxlen=100)\n",
    "\n",
    "    while True:\n",
    "        if total_timesteps >= max_timesteps:\n",
    "            break\n",
    "\n",
    "        # Get the next trajectory segment\n",
    "        seg = seg_gen.__next__()        \n",
    "        obs = np.concatenate(seg[\"ob\"])\n",
    "        actions = np.concatenate(seg[\"ac\"])\n",
    "        old_logprob = np.concatenate(seg[\"logprob\"])\n",
    "        rewards = seg[\"rew\"]\n",
    "        \n",
    "        # Compute discounted future rewards\n",
    "        adv = discounted_rewards_to_go(rewards, discount)\n",
    "        adv = np.expand_dims(adv, axis=1)\n",
    "        \n",
    "        # Normalize the advantages. This lowers variance\n",
    "        adv = (adv - adv.mean()) / (adv.std() + 1e-10)\n",
    "            \n",
    "        # Process each epoch in mini batches. Shuffle between epochs\n",
    "        indices = np.arange(obs.shape[0])\n",
    "        assert(indices.shape[0] % batch_size == 0)\n",
    "        num_batches = indices.shape[0] // batch_size\n",
    "        for e in range(num_epochs):\n",
    "            for i in range(num_batches):\n",
    "                start_index = i*batch_size\n",
    "                end_index = start_index + batch_size\n",
    "                loss = -surrogate_loss(policy,\n",
    "                                       obs[start_index:end_index, :],\n",
    "                                       actions[start_index:end_index, :],\n",
    "                                       adv[start_index:end_index, :],\n",
    "                                       old_logprob[start_index:end_index],\n",
    "                                       epsilon,\n",
    "                                       ent_coef)            \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Shuffle\n",
    "            np.random.shuffle(indices)\n",
    "            obs = obs[indices,:]\n",
    "            actions = actions[indices,:]\n",
    "            adv = adv[indices, :]\n",
    "            old_logprob = old_logprob[indices]\n",
    "\n",
    "        # See if we completed an episode\n",
    "        if seg[\"done\"]:\n",
    "            total_episodes += 1\n",
    "            total_timesteps += seg[\"total_timesteps\"]\n",
    "            ep_rets = seg[\"ep_rets\"]\n",
    "            avg_reward = ep_rets[0] / num_agents\n",
    "            mean_rewards.append(avg_reward)\n",
    "            rewards_buffer.append(avg_reward)\n",
    "            \n",
    "            # Anneal clipping parameter\n",
    "            epsilon *= mult\n",
    "            # Reduce regularization term also reduces (less exploration in later runs)\n",
    "            ent_coef *= 0.995\n",
    "\n",
    "            # Print status as requested\n",
    "            if total_episodes % print_every == 0:\n",
    "                ep_lens = seg[\"ep_lens\"]\n",
    "                print(\"Episode length is {}, average reward is {}, total timesteps is {}\" \\\n",
    "                    .format(ep_lens[0], avg_reward, total_timesteps))\n",
    "                \n",
    "            if np.mean(rewards_buffer) >= 30.0:\n",
    "                print(\"\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}\" \\\n",
    "                      .format(total_episodes - 100, np.mean(rewards_buffer)))\n",
    "                # Save the model\n",
    "                torch.save(policy.state_dict(), 'checkpoint.pth')\n",
    "                break\n",
    "            \n",
    "    return mean_rewards\n",
    "    \n",
    "scores = learn(max_timesteps=600000, horizon=120, batch_size=800, discount=0.99, print_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Plot the rewards\n",
    "\n",
    "Plot the training progress over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "fig.savefig('training.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Play an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]\n",
    "observation = np.asarray(env_info.vector_observations, dtype=np.float32)\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        actions, _ = policy(torch.from_numpy(observation))\n",
    "        actions = torch.clamp(actions, -1, 1).numpy()\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "    observation = np.asarray(env_info.vector_observations, dtype=np.float32)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
